"""
ì ì§„ì  í•™ìŠµ íŒŒì´í”„ë¼ì¸
ì‚¬ìš©ìž í”¼ë“œë°±ê³¼ OCR ê²°ê³¼ë¥¼ ì§€ì†ì ìœ¼ë¡œ í•™ìŠµí•˜ì—¬ ì„±ëŠ¥ í–¥ìƒ
"""

import pandas as pd
import numpy as np
import os
import json
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
import logging

from .database_builder import DatabaseBuilder
from .pattern_generator import PatternGenerator
from .multi_semester_matcher import MultiSemesterMatcher

logger = logging.getLogger(__name__)


class LearningPipeline:
    """ì ì§„ì  í•™ìŠµ íŒŒì´í”„ë¼ì¸"""

    def __init__(self, base_data_dir: str = "data"):
        self.base_data_dir = base_data_dir
        self.training_data_dir = os.path.join(base_data_dir, "training_data")
        self.feedback_file = os.path.join(self.training_data_dir, "user_feedback.xlsx")
        self.corrections_file = os.path.join(self.training_data_dir, "ocr_corrections.xlsx")
        self.learning_log_file = os.path.join(self.training_data_dir, "learning_log.json")

        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.db_builder = DatabaseBuilder()
        self.pattern_generator = PatternGenerator()
        self.matcher = None

        self._ensure_directories()
        self._initialize_files()

    def _ensure_directories(self):
        """í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±"""
        os.makedirs(self.training_data_dir, exist_ok=True)

    def _initialize_files(self):
        """í•™ìŠµ ë°ì´í„° íŒŒì¼ ì´ˆê¸°í™”"""
        # ì‚¬ìš©ìž í”¼ë“œë°± íŒŒì¼ ì´ˆê¸°í™”
        if not os.path.exists(self.feedback_file):
            feedback_df = pd.DataFrame(columns=[
                'timestamp', 'original_ocr', 'user_correction', 'confidence',
                'context', 'feedback_type', 'user_id'
            ])
            feedback_df.to_excel(self.feedback_file, index=False)

        # OCR ë³´ì • íŒŒì¼ ì´ˆê¸°í™”
        if not os.path.exists(self.corrections_file):
            corrections_df = pd.DataFrame(columns=[
                'ocr_result', 'correct_result', 'frequency', 'confidence',
                'error_type', 'last_updated', 'auto_approved'
            ])
            corrections_df.to_excel(self.corrections_file, index=False)

        # í•™ìŠµ ë¡œê·¸ íŒŒì¼ ì´ˆê¸°í™”
        if not os.path.exists(self.learning_log_file):
            initial_log = {
                'last_update': datetime.now().isoformat(),
                'total_corrections': 0,
                'auto_approved_corrections': 0,
                'performance_history': []
            }
            with open(self.learning_log_file, 'w', encoding='utf-8') as f:
                json.dump(initial_log, f, ensure_ascii=False, indent=2)

    def collect_user_feedback(self, ocr_results: List[str], user_corrections: List[str],
                            context: Optional[Dict] = None, user_id: str = "anonymous") -> int:
        """
        ì‚¬ìš©ìž í”¼ë“œë°± ìˆ˜ì§‘

        Args:
            ocr_results: OCR ì›ë³¸ ê²°ê³¼
            user_corrections: ì‚¬ìš©ìž ìˆ˜ì • ê²°ê³¼
            context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ ì •ë³´
            user_id: ì‚¬ìš©ìž ID

        Returns:
            int: ìˆ˜ì§‘ëœ í”¼ë“œë°± ìˆ˜
        """
        if len(ocr_results) != len(user_corrections):
            logger.error("OCR ê²°ê³¼ì™€ ì‚¬ìš©ìž ìˆ˜ì • ê²°ê³¼ì˜ ê¸¸ì´ê°€ ë‹¤ë¦…ë‹ˆë‹¤")
            return 0

        feedback_data = []
        corrections_count = 0

        for ocr_result, user_correction in zip(ocr_results, user_corrections):
            if ocr_result.strip() != user_correction.strip():  # ìˆ˜ì •ì´ ìžˆëŠ” ê²½ìš°ë§Œ
                feedback_data.append({
                    'timestamp': datetime.now().isoformat(),
                    'original_ocr': ocr_result,
                    'user_correction': user_correction,
                    'confidence': self._calculate_edit_confidence(ocr_result, user_correction),
                    'context': json.dumps(context or {}, ensure_ascii=False),
                    'feedback_type': self._classify_feedback_type(ocr_result, user_correction),
                    'user_id': user_id
                })
                corrections_count += 1

        if feedback_data:
            # ê¸°ì¡´ í”¼ë“œë°±ì— ì¶”ê°€
            try:
                existing_feedback = pd.read_excel(self.feedback_file)
                new_feedback = pd.DataFrame(feedback_data)
                combined_feedback = pd.concat([existing_feedback, new_feedback], ignore_index=True)
                combined_feedback.to_excel(self.feedback_file, index=False)

                logger.info(f"ì‚¬ìš©ìž í”¼ë“œë°± {corrections_count}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")

                # ìžë™ í•™ìŠµ íŠ¸ë¦¬ê±°
                self._process_new_feedback(feedback_data)

            except Exception as e:
                logger.error(f"í”¼ë“œë°± ì €ìž¥ ì‹¤íŒ¨: {e}")

        return corrections_count

    def _calculate_edit_confidence(self, original: str, corrected: str) -> float:
        """íŽ¸ì§‘ ì‹ ë¢°ë„ ê³„ì‚°"""
        if original == corrected:
            return 1.0

        # íŽ¸ì§‘ ê±°ë¦¬ ê¸°ë°˜ ì‹ ë¢°ë„
        max_len = max(len(original), len(corrected))
        if max_len == 0:
            return 0.0

        # ê°„ë‹¨í•œ íŽ¸ì§‘ ê±°ë¦¬ ê³„ì‚°
        common_chars = set(original) & set(corrected)
        total_chars = set(original) | set(corrected)

        if not total_chars:
            return 0.0

        similarity = len(common_chars) / len(total_chars)
        return min(0.9, similarity * 1.2)  # ìµœëŒ€ 0.9ë¡œ ì œí•œ

    def _classify_feedback_type(self, original: str, corrected: str) -> str:
        """í”¼ë“œë°± íƒ€ìž… ë¶„ë¥˜"""
        if len(corrected) < len(original):
            return 'truncation_fix'
        elif len(corrected) > len(original):
            return 'completion'
        elif any(c.isalpha() for c in original) != any(c.isalpha() for c in corrected):
            return 'lang_correction'
        else:
            return 'character_correction'

    def _process_new_feedback(self, feedback_data: List[Dict]):
        """ìƒˆë¡œìš´ í”¼ë“œë°± ì²˜ë¦¬ ë° í•™ìŠµ"""
        for feedback in feedback_data:
            self._update_corrections_database(
                feedback['original_ocr'],
                feedback['user_correction'],
                feedback['confidence']
            )

    def _update_corrections_database(self, ocr_result: str, correct_result: str, confidence: float):
        """ë³´ì • ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸"""
        try:
            corrections_df = pd.read_excel(self.corrections_file)

            # ê¸°ì¡´ ë³´ì • ì°¾ê¸°
            existing_mask = (corrections_df['ocr_result'] == ocr_result) & \
                          (corrections_df['correct_result'] == correct_result)

            if existing_mask.any():
                # ê¸°ì¡´ ë³´ì • ì—…ë°ì´íŠ¸ (ë¹ˆë„ ì¦ê°€)
                corrections_df.loc[existing_mask, 'frequency'] += 1
                corrections_df.loc[existing_mask, 'confidence'] = \
                    (corrections_df.loc[existing_mask, 'confidence'] + confidence) / 2
                corrections_df.loc[existing_mask, 'last_updated'] = datetime.now().isoformat()
            else:
                # ìƒˆë¡œìš´ ë³´ì • ì¶”ê°€
                new_correction = {
                    'ocr_result': ocr_result,
                    'correct_result': correct_result,
                    'frequency': 1,
                    'confidence': confidence,
                    'error_type': self._classify_error_type(ocr_result, correct_result),
                    'last_updated': datetime.now().isoformat(),
                    'auto_approved': False
                }
                new_correction_df = pd.DataFrame([new_correction])
                corrections_df = pd.concat([corrections_df, new_correction_df], ignore_index=True)

            corrections_df.to_excel(self.corrections_file, index=False)

        except Exception as e:
            logger.error(f"ë³´ì • ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

    def _classify_error_type(self, original: str, corrected: str) -> str:
        """ì˜¤ë¥˜ íƒ€ìž… ë¶„ë¥˜"""
        if 'ì»´í“¨í„°' in original or 'ì»´í“¨í„°' in corrected:
            return 'subject_name'
        elif original.isdigit() or corrected.isdigit():
            return 'subject_code'
        elif any(name in original.lower() or name in corrected.lower()
                for name in ['êµìˆ˜', 'ë°•ì‚¬', 'ì„ ìƒ']):
            return 'professor_name'
        else:
            return 'general_text'

    def auto_approve_corrections(self, min_frequency: int = 3, min_confidence: float = 0.8) -> int:
        """
        ë¹ˆë„ì™€ ì‹ ë¢°ë„ ê¸°ì¤€ìœ¼ë¡œ ë³´ì • ìžë™ ìŠ¹ì¸

        Args:
            min_frequency: ìµœì†Œ ë¹ˆë„
            min_confidence: ìµœì†Œ ì‹ ë¢°ë„

        Returns:
            int: ìžë™ ìŠ¹ì¸ëœ ë³´ì • ìˆ˜
        """
        try:
            corrections_df = pd.read_excel(self.corrections_file)

            # ìžë™ ìŠ¹ì¸ ì¡°ê±´
            auto_approve_mask = (
                (corrections_df['frequency'] >= min_frequency) &
                (corrections_df['confidence'] >= min_confidence) &
                (~corrections_df['auto_approved'])
            )

            approved_count = auto_approve_mask.sum()

            if approved_count > 0:
                corrections_df.loc[auto_approve_mask, 'auto_approved'] = True
                corrections_df.to_excel(self.corrections_file, index=False)

                # ìŠ¹ì¸ëœ ë³´ì •ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ë°˜ì˜
                self._apply_approved_corrections(corrections_df[auto_approve_mask])

                logger.info(f"{approved_count}ê°œ ë³´ì •ì´ ìžë™ ìŠ¹ì¸ë˜ì—ˆìŠµë‹ˆë‹¤")

            return approved_count

        except Exception as e:
            logger.error(f"ìžë™ ìŠ¹ì¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return 0

    def _apply_approved_corrections(self, approved_corrections: pd.DataFrame):
        """ìŠ¹ì¸ëœ ë³´ì •ì„ í†µí•© ë°ì´í„°ë² ì´ìŠ¤ì— ë°˜ì˜"""
        if self.matcher is None:
            self.matcher = MultiSemesterMatcher()

        # ë³„ëª…/ì•½ì¹­ ì¶”ê°€
        for _, correction in approved_corrections.iterrows():
            original_ocr = correction['ocr_result']
            correct_result = correction['correct_result']

            if correct_result in self.matcher.subject_dict:
                # ê¸°ì¡´ ê³¼ëª©ì˜ ë³„ëª…ì— OCR ì˜¤ë¥˜ íŒ¨í„´ ì¶”ê°€
                current_aliases = self.matcher.alias_dict.get(correct_result, "")
                if original_ocr not in current_aliases:
                    self.matcher.alias_dict[original_ocr] = correct_result

        logger.info("ìŠ¹ì¸ëœ ë³´ì •ì´ ë§¤ì¹­ ì‹œìŠ¤í…œì— ë°˜ì˜ë˜ì—ˆìŠµë‹ˆë‹¤")

    def generate_performance_report(self, days: int = 30) -> Dict:
        """ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±"""
        try:
            # ìµœê·¼ Nì¼ê°„ì˜ í”¼ë“œë°± ë¶„ì„
            feedback_df = pd.read_excel(self.feedback_file)
            feedback_df['timestamp'] = pd.to_datetime(feedback_df['timestamp'])

            cutoff_date = datetime.now() - timedelta(days=days)
            recent_feedback = feedback_df[feedback_df['timestamp'] > cutoff_date]

            # ë³´ì • ë°ì´í„° ë¶„ì„
            corrections_df = pd.read_excel(self.corrections_file)

            report = {
                'period_days': days,
                'total_feedback': len(recent_feedback),
                'total_corrections': len(corrections_df),
                'auto_approved_corrections': len(corrections_df[corrections_df['auto_approved']]),
                'pending_corrections': len(corrections_df[~corrections_df['auto_approved']]),
                'average_confidence': float(corrections_df['confidence'].mean()) if len(corrections_df) > 0 else 0.0,
                'error_type_distribution': corrections_df['error_type'].value_counts().to_dict(),
                'feedback_type_distribution': recent_feedback['feedback_type'].value_counts().to_dict() if len(recent_feedback) > 0 else {},
                'most_common_corrections': corrections_df.nlargest(10, 'frequency')[
                    ['ocr_result', 'correct_result', 'frequency']
                ].to_dict('records'),
                'generated_at': datetime.now().isoformat()
            }

            return report

        except Exception as e:
            logger.error(f"ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return {}

    def daily_learning_routine(self):
        """ì¼ì¼ í•™ìŠµ ë£¨í‹´"""
        logger.info("ì¼ì¼ í•™ìŠµ ë£¨í‹´ ì‹œìž‘")

        # 1. ìžë™ ìŠ¹ì¸ ì²˜ë¦¬
        approved_count = self.auto_approve_corrections()

        # 2. ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±
        report = self.generate_performance_report(days=7)

        # 3. í•™ìŠµ ë¡œê·¸ ì—…ë°ì´íŠ¸
        self._update_learning_log(report, approved_count)

        # 4. ì˜¤ëž˜ëœ ë°ì´í„° ì •ë¦¬
        self._cleanup_old_data(days=90)

        logger.info(f"ì¼ì¼ í•™ìŠµ ë£¨í‹´ ì™„ë£Œ: {approved_count}ê°œ ë³´ì • ìŠ¹ì¸")

    def _update_learning_log(self, report: Dict, approved_count: int):
        """í•™ìŠµ ë¡œê·¸ ì—…ë°ì´íŠ¸"""
        try:
            with open(self.learning_log_file, 'r', encoding='utf-8') as f:
                log_data = json.load(f)

            log_data['last_update'] = datetime.now().isoformat()
            log_data['total_corrections'] = report.get('total_corrections', 0)
            log_data['auto_approved_corrections'] = report.get('auto_approved_corrections', 0)

            # ì„±ëŠ¥ ížˆìŠ¤í† ë¦¬ ì¶”ê°€
            performance_entry = {
                'date': datetime.now().date().isoformat(),
                'total_feedback': report.get('total_feedback', 0),
                'approved_today': approved_count,
                'average_confidence': report.get('average_confidence', 0.0)
            }
            log_data['performance_history'].append(performance_entry)

            # ížˆìŠ¤í† ë¦¬ëŠ” ìµœê·¼ 30ì¼ë§Œ ìœ ì§€
            log_data['performance_history'] = log_data['performance_history'][-30:]

            with open(self.learning_log_file, 'w', encoding='utf-8') as f:
                json.dump(log_data, f, ensure_ascii=False, indent=2)

        except Exception as e:
            logger.error(f"í•™ìŠµ ë¡œê·¸ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

    def _cleanup_old_data(self, days: int):
        """ì˜¤ëž˜ëœ í”¼ë“œë°± ë°ì´í„° ì •ë¦¬"""
        try:
            feedback_df = pd.read_excel(self.feedback_file)
            feedback_df['timestamp'] = pd.to_datetime(feedback_df['timestamp'])

            cutoff_date = datetime.now() - timedelta(days=days)
            recent_feedback = feedback_df[feedback_df['timestamp'] > cutoff_date]

            if len(recent_feedback) < len(feedback_df):
                recent_feedback.to_excel(self.feedback_file, index=False)
                removed_count = len(feedback_df) - len(recent_feedback)
                logger.info(f"{removed_count}ê°œì˜ ì˜¤ëž˜ëœ í”¼ë“œë°± ë°ì´í„°ë¥¼ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤")

        except Exception as e:
            logger.error(f"ë°ì´í„° ì •ë¦¬ ì‹¤íŒ¨: {e}")

    def export_training_data(self, output_dir: str = "data/training_data/exports"):
        """í•™ìŠµ ë°ì´í„° ë‚´ë³´ë‚´ê¸°"""
        os.makedirs(output_dir, exist_ok=True)

        try:
            # 1. ìŠ¹ì¸ëœ ë³´ì • ë°ì´í„°
            corrections_df = pd.read_excel(self.corrections_file)
            approved_corrections = corrections_df[corrections_df['auto_approved']]
            approved_corrections.to_excel(
                os.path.join(output_dir, f"approved_corrections_{datetime.now().date()}.xlsx"),
                index=False
            )

            # 2. ì„±ëŠ¥ ë¦¬í¬íŠ¸
            report = self.generate_performance_report(days=30)
            with open(os.path.join(output_dir, f"performance_report_{datetime.now().date()}.json"),
                     'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)

            # 3. í•™ìŠµ í†µê³„
            stats = {
                'total_corrections': len(corrections_df),
                'approved_corrections': len(approved_corrections),
                'error_types': corrections_df['error_type'].value_counts().to_dict(),
                'export_date': datetime.now().isoformat()
            }

            with open(os.path.join(output_dir, f"learning_stats_{datetime.now().date()}.json"),
                     'w', encoding='utf-8') as f:
                json.dump(stats, f, ensure_ascii=False, indent=2)

            logger.info(f"í•™ìŠµ ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {output_dir}")

        except Exception as e:
            logger.error(f"í•™ìŠµ ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")

    def simulate_learning_data(self, num_samples: int = 100):
        """í•™ìŠµ ë°ì´í„° ì‹œë®¬ë ˆì´ì…˜ (í…ŒìŠ¤íŠ¸ìš©)"""
        # ìƒ˜í”Œ ê³¼ëª©ëª…ë“¤
        sample_subjects = [
            "ì»´í“¨í„°êµ¬ì¡°", "ì˜¤í”ˆì†ŒìŠ¤í”Œëž«í¼", "ìš´ì˜ì²´ì œ", "ë°ì´í„°ë² ì´ìŠ¤",
            "ì•Œê³ ë¦¬ì¦˜", "í”„ë¡œê·¸ëž˜ë°ì–¸ì–´", "ì†Œí”„íŠ¸ì›¨ì–´ê³µí•™", "ë„¤íŠ¸ì›Œí¬"
        ]

        simulated_feedback = []

        for _ in range(num_samples):
            original_subject = np.random.choice(sample_subjects)

            # ì˜¤ë¥˜ íŒ¨í„´ ìƒì„±
            error_patterns = self.pattern_generator.generate_ocr_errors(original_subject, error_rate=0.4)
            if error_patterns:
                ocr_result = np.random.choice(error_patterns)

                simulated_feedback.append({
                    'timestamp': datetime.now().isoformat(),
                    'original_ocr': ocr_result,
                    'user_correction': original_subject,
                    'confidence': np.random.uniform(0.6, 0.95),
                    'context': json.dumps({'simulation': True}, ensure_ascii=False),
                    'feedback_type': 'character_correction',
                    'user_id': 'simulation'
                })

        # ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ì €ìž¥
        if simulated_feedback:
            existing_feedback = pd.read_excel(self.feedback_file)
            new_feedback = pd.DataFrame(simulated_feedback)
            combined_feedback = pd.concat([existing_feedback, new_feedback], ignore_index=True)
            combined_feedback.to_excel(self.feedback_file, index=False)

            logger.info(f"{len(simulated_feedback)}ê°œì˜ ì‹œë®¬ë ˆì´ì…˜ í•™ìŠµ ë°ì´í„° ìƒì„± ì™„ë£Œ")

            # ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°ë¡œ í•™ìŠµ ì‹¤í–‰
            for feedback in simulated_feedback:
                self._update_corrections_database(
                    feedback['original_ocr'],
                    feedback['user_correction'],
                    feedback['confidence']
                )


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ðŸ“š ì ì§„ì  í•™ìŠµ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    # í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    pipeline = LearningPipeline()

    # 1. ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ìƒì„±
    print("ðŸ”§ ì‹œë®¬ë ˆì´ì…˜ í•™ìŠµ ë°ì´í„° ìƒì„± ì¤‘...")
    pipeline.simulate_learning_data(num_samples=50)

    # 2. ìžë™ ìŠ¹ì¸ ì²˜ë¦¬
    print("\nðŸ¤– ìžë™ ìŠ¹ì¸ ì²˜ë¦¬ ì¤‘...")
    approved_count = pipeline.auto_approve_corrections(min_frequency=2, min_confidence=0.7)
    print(f"âœ… {approved_count}ê°œ ë³´ì •ì´ ìžë™ ìŠ¹ì¸ë˜ì—ˆìŠµë‹ˆë‹¤")

    # 3. ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±
    print("\nðŸ“Š ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
    report = pipeline.generate_performance_report(days=30)

    print("ðŸ“ˆ í•™ìŠµ ì‹œìŠ¤í…œ í˜„í™©:")
    print(f"  â€¢ ì´ í”¼ë“œë°±: {report.get('total_feedback', 0)}ê°œ")
    print(f"  â€¢ ì´ ë³´ì •: {report.get('total_corrections', 0)}ê°œ")
    print(f"  â€¢ ìžë™ ìŠ¹ì¸: {report.get('auto_approved_corrections', 0)}ê°œ")
    print(f"  â€¢ í‰ê·  ì‹ ë¢°ë„: {report.get('average_confidence', 0):.3f}")

    if report.get('error_type_distribution'):
        print(f"\nðŸ” ì˜¤ë¥˜ íƒ€ìž…ë³„ ë¶„í¬:")
        for error_type, count in list(report['error_type_distribution'].items())[:5]:
            print(f"  â€¢ {error_type}: {count}ê°œ")

    # 4. ì¼ì¼ í•™ìŠµ ë£¨í‹´ ì‹¤í–‰
    print("\nâš™ï¸ ì¼ì¼ í•™ìŠµ ë£¨í‹´ ì‹¤í–‰ ì¤‘...")
    pipeline.daily_learning_routine()

    # 5. í•™ìŠµ ë°ì´í„° ë‚´ë³´ë‚´ê¸°
    print("\nðŸ’¾ í•™ìŠµ ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì¤‘...")
    pipeline.export_training_data()

    print("\nâœ… ì ì§„ì  í•™ìŠµ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


if __name__ == "__main__":
    main()