"""
OCR ì˜¤ë¥˜ íŒ¨í„´ ìƒì„±ê¸°
ì‹¤ì œ OCRì—ì„œ ë°œìƒí•˜ëŠ” ì˜¤ë¥˜ë¥¼ ì‹œë®¬ë ˆì´ì…˜í•˜ì—¬ í•™ìŠµ ë°ì´í„° ìƒì„±
"""

import random
import re
from typing import List, Dict, Tuple
import pandas as pd
import logging

logger = logging.getLogger(__name__)


class PatternGenerator:
    """OCR ì˜¤ë¥˜ íŒ¨í„´ ìƒì„±ê¸°"""

    def __init__(self):
        # í•œê¸€ ìëª¨ í˜¼ë™ íŒ¨í„´
        self.jamo_confusion = {
            'ã…£': ['|', 'l', '1', 'I'],
            'ã…‡': ['o', 'O', '0'],
            'ã…¡': ['-', '_', 'â€•'],
            'ã„±': ['r', 'ã„´'],
            'ã„´': ['ã„±', 'L'],
            'ã„¹': ['ã„´', 'ã…'],
            'ã…': ['ã…“', 'ã…‘'],
            'ã…“': ['ã…', 'ã…•'],
            'ã…—': ['ã…œ', 'ã…›'],
            'ã…œ': ['ã…—', 'ã… ']
        }

        # ì˜ë¬¸-í•œê¸€ í˜¼ë™ íŒ¨í„´
        self.eng_kor_confusion = {
            'a': ['ã…', 'ã…‘'],
            'o': ['ã…‡', 'ã…—'],
            'i': ['ã…£', 'ã…'],
            'u': ['ã…œ', 'ã… '],
            'e': ['ã…“', 'ã…”'],
            'l': ['ã…£', '|'],
            'r': ['ã„±', 'ã„´'],
            'n': ['ã„´', 'ã…'],
            's': ['ã……', 'ã…†'],
            't': ['ã…Œ', 'ã…œ']
        }

        # ìœ ì‚¬ í•œê¸€ ê¸€ì í˜¼ë™
        self.similar_hangul = {
            'ê°€': ['ê¹Œ', 'ë‚˜', 'ë‹¤'],
            'ë‚˜': ['ë‹¤', 'ë¼', 'ë§ˆ'],
            'ë‹¤': ['ë¼', 'ë§ˆ', 'ë°”'],
            'ë¼': ['ë§ˆ', 'ë°”', 'ì‚¬'],
            'ë§ˆ': ['ë°”', 'ì‚¬', 'ì•„'],
            'ë°”': ['ì‚¬', 'ì•„', 'ì'],
            'ì‚¬': ['ì•„', 'ì', 'ì°¨'],
            'ì•„': ['ì', 'ì°¨', 'ì¹´'],
            'ì': ['ì°¨', 'ì¹´', 'íƒ€'],
            'ì°¨': ['ì¹´', 'íƒ€', 'íŒŒ'],
            'êµ¬': ['ë¶€', 'ìˆ˜', 'ëˆ„'],
            'ì¡°': ['ì´ˆ', 'ì†Œ', 'ë„'],
            'ì„¤': ['ì„¤', 'ì ˆ', 'í„¸'],
            'ê³„': ['ê²Œ', 'ê°œ', 'ê»˜']
        }

        # ê³¼ëª©ëª…ì—ì„œ ìì£¼ ë°œìƒí•˜ëŠ” ì˜¤ë¥˜ íŒ¨í„´
        self.subject_specific_errors = {
            'ì»´í“¨í„°': ['ì»´í“¨í„°', 'ì»´í“¨íƒ€', 'ì»´í“¨í„°'],
            'í”„ë¡œê·¸ë˜ë°': ['í”„ë¡œê·¸ë˜ë°', 'í”„ë¡œê·¸ë¦¬ë°', 'í”„ë¡œê·¸ë¨ë°'],
            'ë°ì´í„°ë² ì´ìŠ¤': ['ë°ì´í„°ë² ì´ìŠ¤', 'ë°ì´íƒ€ë² ì´ìŠ¤', 'ë°ì´í„°ë°°ì´ìŠ¤'],
            'ìš´ì˜ì²´ì œ': ['ìš´ì˜ì²´ê³„', 'ìš´ì˜ì²´ì¬', 'ìš´ì˜ì²´ì²´'],
            'ì•Œê³ ë¦¬ì¦˜': ['ì•Œê³ ë¦¬ì¦˜', 'ì•Œê³ ë¦¬ë“¬', 'ì•Œê³ ì´ë“¬'],
            'ë„¤íŠ¸ì›Œí¬': ['ë„¤íŠ¸ì›Œí¬', 'ë„¤íŠ¸ì›Œí¬', 'ë„·ì›Œí¬'],
            'ì†Œí”„íŠ¸ì›¨ì–´': ['ì†Œí”„íŠ¸ì›¨ì–´', 'ì†Œí”„íŠ¸ì›¨ì–´', 'ì†Œí”„íŠ¸ì›¨ì•„'],
            'ì¸ê³µì§€ëŠ¥': ['ì¸ê³µì§€ëŠ¥', 'ì¸ê³µì§€ë¦‰', 'ì¸ê³µì§€ë‡½'],
            'ì‹œìŠ¤í…œ': ['ì‹œìŠ¤í…œ', 'ì‹œìŠ¤íƒ¬', 'ì‹œìŠ¤ë€'],
            'êµ¬ì¡°': ['êµ¬ì¡°', 'êµ¬ì´ˆ', 'êµ¬ì£ '],
            'ì„¤ê³„': ['ì„¤ê³„', 'ì„¤ê²Œ', 'ì ˆê³„'],
            'ë¶„ì„': ['ë¶„ì„', 'ë¶„ì„œ', 'ë¶„ì„¤'],
            'ê°œë¡ ': ['ê°œë¡ ', 'ê°œë£¬', 'ê°œë¥¸'],
            'ê³µí•™': ['ê³µí•™', 'ê³µì•…', 'ê³µí™•'],
            'ì´ë¡ ': ['ì´ë¡ ', 'ì´ë£¬', 'ì´ë¥¸']
        }

    def generate_ocr_errors(self, text: str, error_rate: float = 0.3) -> List[str]:
        """
        ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì—ì„œ OCR ì˜¤ë¥˜ íŒ¨í„´ì„ ìƒì„±

        Args:
            text: ì›ë³¸ í…ìŠ¤íŠ¸
            error_rate: ì˜¤ë¥˜ ë°œìƒ ë¹„ìœ¨ (0.0 ~ 1.0)

        Returns:
            List[str]: ë‹¤ì–‘í•œ OCR ì˜¤ë¥˜ ë²„ì „ë“¤
        """
        error_versions = [text]  # ì›ë³¸ë„ í¬í•¨

        # 1. ìëª¨ ë¶„ë¦¬/í˜¼ë™ ì˜¤ë¥˜
        error_versions.extend(self._generate_jamo_errors(text, error_rate))

        # 2. ì˜ë¬¸-í•œê¸€ í˜¼ë™ ì˜¤ë¥˜
        error_versions.extend(self._generate_eng_kor_errors(text, error_rate))

        # 3. ìœ ì‚¬ ê¸€ì í˜¼ë™ ì˜¤ë¥˜
        error_versions.extend(self._generate_similar_char_errors(text, error_rate))

        # 4. ê³¼ëª©ëª… íŠ¹í™” ì˜¤ë¥˜
        error_versions.extend(self._generate_subject_specific_errors(text))

        # 5. ê³µë°±/íŠ¹ìˆ˜ë¬¸ì ì˜¤ë¥˜
        error_versions.extend(self._generate_spacing_errors(text))

        # 6. ë¶€ë¶„ ì¸ì‹ ì˜¤ë¥˜
        error_versions.extend(self._generate_partial_recognition_errors(text))

        # ì¤‘ë³µ ì œê±°
        return list(set(error_versions))

    def _generate_jamo_errors(self, text: str, error_rate: float) -> List[str]:
        """ìëª¨ ë¶„ë¦¬/í˜¼ë™ ì˜¤ë¥˜ ìƒì„±"""
        errors = []

        for _ in range(int(len(text) * error_rate) + 1):
            modified_text = text
            for jamo, confusions in self.jamo_confusion.items():
                if jamo in modified_text and random.random() < error_rate:
                    replacement = random.choice(confusions)
                    modified_text = modified_text.replace(jamo, replacement, 1)

            if modified_text != text:
                errors.append(modified_text)

        return errors

    def _generate_eng_kor_errors(self, text: str, error_rate: float) -> List[str]:
        """ì˜ë¬¸-í•œê¸€ í˜¼ë™ ì˜¤ë¥˜ ìƒì„±"""
        errors = []

        # í•œê¸€ì„ ì˜ë¬¸ìœ¼ë¡œ
        modified_text = text
        for kor, eng_list in self.eng_kor_confusion.items():
            for eng in eng_list:
                if eng in modified_text and random.random() < error_rate:
                    modified_text = modified_text.replace(eng, kor, 1)

        if modified_text != text:
            errors.append(modified_text)

        # ì˜ë¬¸ì„ í•œê¸€ë¡œ
        modified_text = text
        for eng, kor_list in self.eng_kor_confusion.items():
            if eng in modified_text and random.random() < error_rate:
                replacement = random.choice(kor_list)
                modified_text = modified_text.replace(eng, replacement, 1)

        if modified_text != text:
            errors.append(modified_text)

        return errors

    def _generate_similar_char_errors(self, text: str, error_rate: float) -> List[str]:
        """ìœ ì‚¬ ê¸€ì í˜¼ë™ ì˜¤ë¥˜ ìƒì„±"""
        errors = []

        for char, similar_chars in self.similar_hangul.items():
            if char in text and random.random() < error_rate:
                for similar in similar_chars[:2]:  # ìƒìœ„ 2ê°œë§Œ
                    modified_text = text.replace(char, similar, 1)
                    errors.append(modified_text)

        return errors

    def _generate_subject_specific_errors(self, text: str) -> List[str]:
        """ê³¼ëª©ëª… íŠ¹í™” ì˜¤ë¥˜ ìƒì„±"""
        errors = []

        for subject, error_variants in self.subject_specific_errors.items():
            if subject in text:
                for variant in error_variants:
                    if variant != subject:
                        modified_text = text.replace(subject, variant)
                        errors.append(modified_text)

        return errors

    def _generate_spacing_errors(self, text: str) -> List[str]:
        """ê³µë°±/íŠ¹ìˆ˜ë¬¸ì ì˜¤ë¥˜ ìƒì„±"""
        errors = []

        # ê³µë°± ì œê±°
        errors.append(text.replace(' ', ''))

        # ê³µë°± ì¶”ê°€
        if len(text) > 2:
            mid_pos = len(text) // 2
            errors.append(text[:mid_pos] + ' ' + text[mid_pos:])

        # íŠ¹ìˆ˜ë¬¸ì í˜¼ë™
        special_char_errors = text
        special_char_errors = special_char_errors.replace('-', '_')
        special_char_errors = special_char_errors.replace('(', '[')
        special_char_errors = special_char_errors.replace(')', ']')

        if special_char_errors != text:
            errors.append(special_char_errors)

        return errors

    def _generate_partial_recognition_errors(self, text: str) -> List[str]:
        """ë¶€ë¶„ ì¸ì‹ ì˜¤ë¥˜ ìƒì„± (ì•ë’¤ ì˜ë¦¼)"""
        errors = []

        if len(text) > 3:
            # ì•ë¶€ë¶„ ì˜ë¦¼
            errors.append(text[1:])
            errors.append(text[2:])

            # ë’·ë¶€ë¶„ ì˜ë¦¼
            errors.append(text[:-1])
            errors.append(text[:-2])

            # ì¤‘ê°„ ë¶€ë¶„ë§Œ
            if len(text) > 5:
                start = len(text) // 4
                end = len(text) * 3 // 4
                errors.append(text[start:end])

        return errors

    def generate_training_dataset(self, subjects: List[str], samples_per_subject: int = 10) -> pd.DataFrame:
        """
        ê³¼ëª© ëª©ë¡ìœ¼ë¡œë¶€í„° í•™ìŠµ ë°ì´í„°ì…‹ ìƒì„±

        Args:
            subjects: ê³¼ëª©ëª… ë¦¬ìŠ¤íŠ¸
            samples_per_subject: ê³¼ëª©ë‹¹ ìƒì„±í•  ìƒ˜í”Œ ìˆ˜

        Returns:
            pd.DataFrame: í•™ìŠµ ë°ì´í„°ì…‹ (original, corrupted, error_type)
        """
        training_data = []

        for subject in subjects:
            logger.info(f"'{subject}' ì˜¤ë¥˜ íŒ¨í„´ ìƒì„± ì¤‘...")

            # ë‹¤ì–‘í•œ ì˜¤ë¥˜ íŒ¨í„´ ìƒì„±
            error_versions = self.generate_ocr_errors(subject, error_rate=0.4)

            # ìƒ˜í”Œ ìˆ˜ë§Œí¼ ì„ íƒ
            selected_errors = error_versions[:samples_per_subject]

            for error_text in selected_errors:
                error_type = self._classify_error_type(subject, error_text)

                training_data.append({
                    'original': subject,
                    'corrupted': error_text,
                    'error_type': error_type,
                    'similarity': self._calculate_similarity(subject, error_text)
                })

        df = pd.DataFrame(training_data)
        logger.info(f"í•™ìŠµ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ: {len(df)}ê°œ ìƒ˜í”Œ")

        return df

    def _classify_error_type(self, original: str, corrupted: str) -> str:
        """ì˜¤ë¥˜ íƒ€ì… ë¶„ë¥˜"""
        if original == corrupted:
            return 'original'

        # ê¸¸ì´ ë¹„êµ
        if len(corrupted) < len(original):
            return 'truncation'
        elif len(corrupted) > len(original):
            return 'insertion'

        # ë¬¸ì íƒ€ì… ë¶„ì„
        if re.search(r'[a-zA-Z]', corrupted) and not re.search(r'[a-zA-Z]', original):
            return 'eng_kor_confusion'

        # ê³µë°± ê´€ë ¨
        if ' ' in original and ' ' not in corrupted:
            return 'space_removal'
        elif ' ' not in original and ' ' in corrupted:
            return 'space_insertion'

        # ìëª¨ í˜¼ë™
        for jamo in self.jamo_confusion.keys():
            if jamo in original and jamo not in corrupted:
                return 'jamo_confusion'

        return 'character_substitution'

    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """ë‘ í…ìŠ¤íŠ¸ ê°„ ìœ ì‚¬ë„ ê³„ì‚° (ê°„ë‹¨í•œ ë²„ì „)"""
        if text1 == text2:
            return 1.0

        # ë¬¸ì ë‹¨ìœ„ ìœ ì‚¬ë„
        common_chars = set(text1) & set(text2)
        total_chars = set(text1) | set(text2)

        if not total_chars:
            return 0.0

        return len(common_chars) / len(total_chars)

    def save_training_dataset(self, subjects: List[str], output_path: str = "data/training_data/ocr_error_patterns.xlsx"):
        """í•™ìŠµ ë°ì´í„°ì…‹ì„ íŒŒì¼ë¡œ ì €ì¥"""
        dataset = self.generate_training_dataset(subjects)
        dataset.to_excel(output_path, index=False)
        logger.info(f"í•™ìŠµ ë°ì´í„°ì…‹ ì €ì¥ ì™„ë£Œ: {output_path}")

        # í†µê³„ ì •ë³´ ì¶œë ¥
        print(f"\nğŸ“Š ìƒì„±ëœ í•™ìŠµ ë°ì´í„° í†µê³„:")
        print(f"ì´ ìƒ˜í”Œ ìˆ˜: {len(dataset)}")
        print(f"ê³ ìœ  ì›ë³¸ ê³¼ëª© ìˆ˜: {dataset['original'].nunique()}")
        print(f"í‰ê·  ìœ ì‚¬ë„: {dataset['similarity'].mean():.3f}")

        print(f"\nğŸ“ˆ ì˜¤ë¥˜ íƒ€ì…ë³„ ë¶„í¬:")
        error_type_counts = dataset['error_type'].value_counts()
        for error_type, count in error_type_counts.items():
            print(f"  â€¢ {error_type}: {count}ê°œ ({count/len(dataset)*100:.1f}%)")

        return dataset

    def test_pattern_generation(self, test_subjects: List[str] = None):
        """íŒ¨í„´ ìƒì„± í…ŒìŠ¤íŠ¸"""
        if test_subjects is None:
            test_subjects = [
                "ì»´í“¨í„°êµ¬ì¡°",
                "ì˜¤í”ˆì†ŒìŠ¤í”Œë«í¼",
                "ìš´ì˜ì²´ì œ",
                "ë°ì´í„°ë² ì´ìŠ¤",
                "ì•Œê³ ë¦¬ì¦˜",
                "í”„ë¡œê·¸ë˜ë°ì–¸ì–´"
            ]

        print("ğŸ§ª OCR ì˜¤ë¥˜ íŒ¨í„´ ìƒì„± í…ŒìŠ¤íŠ¸")
        print("=" * 50)

        for subject in test_subjects:
            print(f"\nğŸ“ ì›ë³¸: {subject}")
            errors = self.generate_ocr_errors(subject, error_rate=0.5)

            print(f"ìƒì„±ëœ ì˜¤ë¥˜ íŒ¨í„´ ({len(errors)}ê°œ):")
            for i, error in enumerate(errors[:8], 1):  # ìƒìœ„ 8ê°œë§Œ ì¶œë ¥
                if error != subject:
                    similarity = self._calculate_similarity(subject, error)
                    error_type = self._classify_error_type(subject, error)
                    print(f"  {i}. {error} (ìœ ì‚¬ë„: {similarity:.2f}, íƒ€ì…: {error_type})")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ”§ OCR ì˜¤ë¥˜ íŒ¨í„´ ìƒì„±ê¸° í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    # íŒ¨í„´ ìƒì„±ê¸° ì´ˆê¸°í™”
    generator = PatternGenerator()

    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    generator.test_pattern_generation()

    # ìƒ˜í”Œ ê³¼ëª©ìœ¼ë¡œ í•™ìŠµ ë°ì´í„° ìƒì„± (ì‹¤ì œë¡œëŠ” DBì—ì„œ ê°€ì ¸ì˜¬ ì˜ˆì •)
    sample_subjects = [
        "ì»´í“¨í„°êµ¬ì¡°", "ì˜¤í”ˆì†ŒìŠ¤í”Œë«í¼", "ìš´ì˜ì²´ì œ", "ë°ì´í„°ë² ì´ìŠ¤",
        "ì•Œê³ ë¦¬ì¦˜", "í”„ë¡œê·¸ë˜ë°ì–¸ì–´", "ì†Œí”„íŠ¸ì›¨ì–´ê³µí•™", "ë„¤íŠ¸ì›Œí¬",
        "ì¸ê³µì§€ëŠ¥", "ì»´í“¨í„°ê·¸ë˜í”½ìŠ¤", "ì •ë³´ë³´ì•ˆ", "ëª¨ë°”ì¼í”„ë¡œê·¸ë˜ë°"
    ]

    print(f"\nğŸ“š {len(sample_subjects)}ê°œ ê³¼ëª©ìœ¼ë¡œ í•™ìŠµ ë°ì´í„° ìƒì„± ì¤‘...")
    generator.save_training_dataset(sample_subjects)

    print("\nâœ… OCR ì˜¤ë¥˜ íŒ¨í„´ ìƒì„± ì™„ë£Œ!")


if __name__ == "__main__":
    main()